{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a13813c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from itertools import permutations, combinations\n",
    "from zipfile import ZipFile\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "s=set(stopwords.words('english'))\n",
    "\n",
    "start = 'C:/Users/gony/Desktop/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef2a111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text):\n",
    "    return list(filter(lambda w: not w.lower() in s,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ddd93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def create_nodes_edges(data):\n",
    "    # creating emtpy df\n",
    "    output_graph = pd.DataFrame(columns=['source', 'target'])\n",
    "    \n",
    "    # iterating over each row \n",
    "    for i, row in tqdm(data.iterrows()):\n",
    "        temp = row['words_sentences']\n",
    "        \n",
    "        # iterating over each sentence, since we are creating an each between two words\n",
    "        # if the appeared together in the same sentence\n",
    "        for sen in temp:\n",
    "            pos_tags = pos_tag(sen)\n",
    "            # creates list of permutation of 2 for all the words in the sentence\n",
    "            lemmatized_words = []\n",
    "            for word, pos in pos_tags:\n",
    "                if pos.startswith('V'):  # Verbs\n",
    "                    lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "                elif pos.startswith('N'):  # Nouns\n",
    "                    lemma = lemmatizer.lemmatize(word, pos='n')\n",
    "                else:\n",
    "                    lemma = word  # Keep the word as is\n",
    "                lemmatized_words.append(lemma)\n",
    "\n",
    "            # Join the lemmatized words back into a sentence\n",
    "            # creates list of permutation of 2 for all the words in the sentence\n",
    "            temp_df = pd.DataFrame(data =list(combinations([x.lower() for x in lemmatized_words], 2)), columns=['source', 'target'])\n",
    "            output_graph = output_graph.append(temp_df, ignore_index = True)\n",
    "    \n",
    "#     # groupby source and target and count all the apperance\n",
    "#     output_graph['count']=output_graph.groupby('source')['target'].transform('count')#all rows ; \n",
    "    count_series = output_graph.groupby(['source', 'target']).size()\n",
    "\n",
    "#     # remove duplicates\n",
    "#     output_graph.drop_duplicates(inplace=True)\n",
    "    new_df = count_series.to_frame(name = 'weight').reset_index()\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e84bf297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [01:49,  1.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "110it [02:27,  1.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "115it [03:05,  1.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "145it [03:04,  1.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120it [02:46,  1.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [02:29,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [02:43,  5.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "93it [03:34,  2.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [03:03,  1.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [03:30,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "117it [02:32,  1.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100it [02:12,  1.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "83it [02:14,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [01:07,  1.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "89it [02:12,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "91it [03:07,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "92it [02:42,  1.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "80it [01:24,  1.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "102it [01:37,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [01:22,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.09.2001.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "54it [00:46,  1.15it/s]\n"
     ]
    }
   ],
   "source": [
    "path = start+'data/9_11_2001.zip'\n",
    "with ZipFile(path, \"r\") as zip_ref:\n",
    "    for f in zip_ref.namelist():\n",
    "        idx = [i for i, ltr in enumerate(f) if ltr == '.']\n",
    "        num_day = int(f[:idx[0]])\n",
    "        num_month = int(f[idx[0]+1:idx[1]])\n",
    "        if (num_month==9 and num_day>=6 and num_day<=26):\n",
    "            print(f)\n",
    "            day = pd.read_csv(zip_ref.open(f))\n",
    "\n",
    "            day['temp'] = day.apply(lambda row: re.sub(r'\\b[A-Z]+\\b', '', row['text']), axis=1)\n",
    "            # split each text by sentences, receiving list of strings each string is a sentence\n",
    "            # in the original text.\n",
    "            day['sentences'] = day.temp.str.split('.')\n",
    "\n",
    "            # removing punctiuation marks from each sentence\n",
    "            day['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in day.sentences]\n",
    "\n",
    "            # split each sentence to words, reveiving list of lists,\n",
    "            # each sublist has all the words in  original sentence after removing stopwords\n",
    "            day['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in day.sentences]\n",
    "\n",
    "            # splitting all words in text to a list of words removing stopwords\n",
    "            # (i forgot to remove punctuation marks but for now we aren't using this so i'll fix in later)\n",
    "            day['words_text'] = [remove_stopwords(t.split()) for t in day.temp]\n",
    "        #     print(Counter(day['words_text'][0]).most_common(5))\n",
    "        #     print(day['sentences'][0][0])\n",
    "        #     break\n",
    "            create_nodes_edges(day).to_csv(start+'data/new/9_11_2001/nodes' + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b182bdf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "78it [03:17,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [01:44,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [02:10,  2.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [01:40,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "65it [02:01,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "86it [03:30,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [04:26,  3.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "64it [03:47,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [06:29,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [03:52,  9.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "55it [05:34,  6.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69it [04:02,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99it [04:42,  2.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "74it [03:21,  2.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "72it [03:23,  2.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [04:50,  6.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [03:47,  4.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.03.2003.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "66it [01:33,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "path = start+'data/iraq_3_20_2003.zip'\n",
    "with ZipFile(path, \"r\") as zip_ref:\n",
    "    for f in zip_ref.namelist():\n",
    "        idx = [i for i, ltr in enumerate(f) if ltr == '.']\n",
    "        num_day = int(f[:idx[0]])\n",
    "        num_month = int(f[idx[0]+1:idx[1]])\n",
    "        if ((num_month==3 and num_day>=16) or (num_month==4 and num_day<=2)) and not os.path.exists(start+'data/new/iraq_3_20_2003/nodes'+f):\n",
    "            print(f)\n",
    "            day = pd.read_csv(zip_ref.open(f))\n",
    "            day['sentences'] = day.text.str.split('.')\n",
    "            day = day[day['sentences'].notna()]\n",
    "            day['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in day.sentences]\n",
    "            day['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in day.sentences]\n",
    "            day['words_text'] = [remove_stopwords(t.split()) for t in day.text]\n",
    "            create_nodes_edges(day).to_csv(start+'data/new/iraq_3_20_2003/nodes' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42137d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [02:23,  5.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [01:46,  4.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [02:09,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [02:32,  6.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:03,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:34,  2.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:32,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:58,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [01:50,  3.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [02:33,  4.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:44,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [01:16,  3.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [01:06,  4.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [02:19,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [00:48,  2.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [02:20,  4.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31.08.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [01:19,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:46,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [01:42,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.09.2005.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [01:52,  3.88s/it]\n"
     ]
    }
   ],
   "source": [
    "path = start + 'data/katrina_08_2005.zip'\n",
    "with ZipFile(path, \"r\") as zip_ref:\n",
    "    for f in zip_ref.namelist():\n",
    "        idx = [i for i, ltr in enumerate(f) if ltr == '.']\n",
    "        num_day = int(f[:idx[0]])\n",
    "        num_month = int(f[idx[0]+1:idx[1]])\n",
    "        if ((num_month==9 and num_day<=6) or (num_month==8 and num_day>=18)):\n",
    "            print(f)\n",
    "            day = pd.read_csv(zip_ref.open(f))\n",
    "            day['sentences'] = day.text.str.split('.')\n",
    "            day = day[day['sentences'].notna()]\n",
    "            day['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in day.sentences]\n",
    "            day['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in day.sentences]\n",
    "            day['words_text'] = [remove_stopwords(t.split()) for t in day.text]\n",
    "            create_nodes_edges(day).to_csv(start+'data/new/katrina_08_2005/nodes' + f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2cad9efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:44,  6.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [05:05, 11.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [05:41, 12.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [05:34, 11.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [05:15, 12.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:27,  7.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:55,  8.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [05:18, 12.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [05:00, 11.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [09:13, 14.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.04.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [05:38, 12.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.04.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [05:33, 12.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.04.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [05:45, 12.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [04:52, 10.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.04.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [02:14,  7.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [05:17, 11.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [05:05, 11.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [04:59, 11.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [02:15,  6.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:16,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.05.2011.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [03:16,  7.55s/it]\n"
     ]
    }
   ],
   "source": [
    "path = start+'data/binladin_02_05_2011/'\n",
    "for f in os.listdir(path):\n",
    "    idx = [i for i, ltr in enumerate(f) if ltr == '.']\n",
    "    num_day = int(f[:idx[0]])\n",
    "    num_month = int(f[idx[0]+1:idx[1]])\n",
    "    if ((num_month==5 and num_day<=17) or (num_month==4 and num_day>=27)):\n",
    "        print(f)\n",
    "        day = pd.read_csv(path+f)\n",
    "        day['sentences'] = day.text.str.split('.')\n",
    "        day = day[day['sentences'].notna()]\n",
    "        day['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in day.sentences]\n",
    "        day['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in day.sentences]\n",
    "        day['words_text'] = [remove_stopwords(t.split()) for t in day.text]\n",
    "        create_nodes_edges(day).to_csv(start+'data/new/binladin_02_05_2011/nodes' + f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b126fb1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [03:18,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [03:37,  5.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [01:48,  5.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:18,  5.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [04:22,  6.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [05:27,  6.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [03:52,  5.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [05:00,  6.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [05:45,  8.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [02:23,  6.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [01:45,  6.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [03:47,  5.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [04:01,  5.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [03:28,  5.34s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [03:53,  5.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [03:16,  5.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [01:37,  4.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [01:13,  5.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [03:30,  5.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.04.2013.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [02:58,  4.58s/it]\n"
     ]
    }
   ],
   "source": [
    "path = start+'data/boston_15_04_2013/'\n",
    "for f in os.listdir(path):\n",
    "    idx = [i for i, ltr in enumerate(f) if ltr == '.']\n",
    "    num_day = int(f[:idx[0]])\n",
    "    num_month = int(f[idx[0]+1:idx[1]])\n",
    "    if (num_month==4 and (num_day>= 11 or num_day<=30)):\n",
    "        print(f)\n",
    "        day = pd.read_csv(path+f)\n",
    "        day['sentences'] = day.text.str.split('.')\n",
    "        day = day[day['sentences'].notna()]\n",
    "        day['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in day.sentences]\n",
    "        day['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in day.sentences]\n",
    "        day['words_text'] = [remove_stopwords(t.split()) for t in day.text]\n",
    "        create_nodes_edges(day).to_csv(start+'data/new/boston_15_04_2013/nodes' + f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "'john'"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stemmer.stem(\"John\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(start+'data/boston_15_04_2013/11.04.2013.csv')\n",
    "data['sentences'] = data.text.str.split('.')\n",
    "data = data[data['sentences'].notna()]\n",
    "data['sentences'] = [[s.translate(str.maketrans('', '', string.punctuation)) for s in t] for t in data.sentences]\n",
    "data['words_sentences'] =[[list(filter(lambda w: not w.lower() in s,sen.split())) for sen in text] for text in data.sentences]\n",
    "data['words_text'] = [remove_stopwords(t.split()) for t in data.text]\n",
    "for i, row in tqdm(data.iterrows()):\n",
    "    temp = row['words_sentences']\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ANDERSON', 'COOPER', 'CNN', 'ANCHOR', 'Good', 'evening', 'everyone']\n",
      "[('ANDERSON', 'NNP'), ('COOPER', 'NNP'), ('CNN', 'NNP'), ('ANCHOR', 'NNP'), ('Good', 'NNP'), ('evening', 'NN'), ('everyone', 'NN')]\n",
      "['ANDERSON', 'COOPER', 'CNN', 'ANCHOR', 'Good', 'evening', 'everyone']\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
